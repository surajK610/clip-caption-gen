# Clip-Caption-Gen

## Overview
This repository contains the implementation of experiments conducted for the Mini-Project 1: Multimodal Learning, as part of the Advanced Topics in Deep Learning course at Brown University. The project explores image captioning models, integrating CLIP with GPT-3, and experiments with a decoder-only transformer architecture.

## Experiments
1. **Zero-Shot Classification with CLIP**: Classification on CIFAR-10 using CLIP.
2. **Linear Probing**: Probing CLIP embeddings for improved image classification.
3. **Socratic Method for Image Captioning**: A two-step process involving CLIP and GPT-3 for generating image captions.
4. **Captioning Model Improvements with DeCap**: Implementing and testing the DeCap model for better captioning performance.
